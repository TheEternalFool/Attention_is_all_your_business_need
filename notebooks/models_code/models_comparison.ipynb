{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14c82a0-c2b3-4c0a-9677-a96392575a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jayyy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jayyy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_89e4e th {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_89e4e td {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_89e4e_row0_col0, #T_89e4e_row0_col1, #T_89e4e_row0_col2, #T_89e4e_row0_col3, #T_89e4e_row1_col0, #T_89e4e_row1_col1, #T_89e4e_row1_col2, #T_89e4e_row1_col3, #T_89e4e_row2_col0, #T_89e4e_row2_col1, #T_89e4e_row2_col2, #T_89e4e_row2_col3, #T_89e4e_row3_col0, #T_89e4e_row3_col1, #T_89e4e_row3_col2, #T_89e4e_row3_col3, #T_89e4e_row4_col0, #T_89e4e_row4_col1, #T_89e4e_row4_col2, #T_89e4e_row4_col3, #T_89e4e_row5_col0, #T_89e4e_row5_col1, #T_89e4e_row5_col2, #T_89e4e_row5_col3, #T_89e4e_row6_col0, #T_89e4e_row6_col1, #T_89e4e_row6_col2, #T_89e4e_row6_col3, #T_89e4e_row7_col0, #T_89e4e_row7_col1, #T_89e4e_row7_col2, #T_89e4e_row7_col3, #T_89e4e_row8_col0, #T_89e4e_row8_col1, #T_89e4e_row8_col2, #T_89e4e_row8_col3, #T_89e4e_row9_col0, #T_89e4e_row9_col1, #T_89e4e_row9_col2, #T_89e4e_row9_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_89e4e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_89e4e_level0_col0\" class=\"col_heading level0 col0\" >Review</th>\n",
       "      <th id=\"T_89e4e_level0_col1\" class=\"col_heading level0 col1\" >Expected Sentiment</th>\n",
       "      <th id=\"T_89e4e_level0_col2\" class=\"col_heading level0 col2\" >Fine-tuned BERT</th>\n",
       "      <th id=\"T_89e4e_level0_col3\" class=\"col_heading level0 col3\" >From-scratch Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_89e4e_row0_col0\" class=\"data row0 col0\" >The smartphone's camera capabilities are truly remarkable, capturing vivid colors and sharp details even in low-light conditions. The user interface is intuitive and the battery life exceeds my expectations, lasting well into the next day with heavy usage.</td>\n",
       "      <td id=\"T_89e4e_row0_col1\" class=\"data row0 col1\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row0_col2\" class=\"data row0 col2\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row0_col3\" class=\"data row0 col3\" >Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_89e4e_row1_col0\" class=\"data row1 col0\" >I attended a weekend workshop on digital marketing, and it was a game-changer for my business. The speakers were industry experts who shared invaluable insights and practical strategies that I could implement immediately.</td>\n",
       "      <td id=\"T_89e4e_row1_col1\" class=\"data row1 col1\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row1_col2\" class=\"data row1 col2\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row1_col3\" class=\"data row1 col3\" >Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_89e4e_row2_col0\" class=\"data row2 col0\" >The cruise vacation was an unforgettable experience. The ship was luxurious, with a variety of dining options, entertainment, and activities for all ages. The staff were incredibly attentive, and the shore excursions were well-planned and enriching.</td>\n",
       "      <td id=\"T_89e4e_row2_col1\" class=\"data row2 col1\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row2_col2\" class=\"data row2 col2\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row2_col3\" class=\"data row2 col3\" >Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_89e4e_row3_col0\" class=\"data row3 col0\" >The custom-built gaming PC I ordered exceeded my expectations in every way. The build quality is impeccable, with meticulously routed cables and high-end components that deliver top-tier performance in every game I've played so far.</td>\n",
       "      <td id=\"T_89e4e_row3_col1\" class=\"data row3 col1\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row3_col2\" class=\"data row3 col2\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row3_col3\" class=\"data row3 col3\" >Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_89e4e_row4_col0\" class=\"data row4 col0\" >The electric car not only provides a smooth and quiet ride but also offers impressive acceleration and a range that easily meets my daily commuting needs. The advanced driver-assistance features enhance safety, making every journey a pleasure.</td>\n",
       "      <td id=\"T_89e4e_row4_col1\" class=\"data row4 col1\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row4_col2\" class=\"data row4 col2\" >Positive</td>\n",
       "      <td id=\"T_89e4e_row4_col3\" class=\"data row4 col3\" >Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_89e4e_row5_col0\" class=\"data row5 col0\" >The new version of the software is riddled with bugs that significantly hamper productivity. It crashes frequently, and the user interface changes have made navigation cumbersome. The promised new features are either missing or don't work as advertised.</td>\n",
       "      <td id=\"T_89e4e_row5_col1\" class=\"data row5 col1\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row5_col2\" class=\"data row5 col2\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row5_col3\" class=\"data row5 col3\" >Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_89e4e_row6_col0\" class=\"data row6 col0\" >I recently stayed at a highly-rated resort, but my experience was far from pleasant. The room was not cleaned properly, with visible dust and stains on the sheets. The food at the on-site restaurant was mediocre, and the staff seemed indifferent to guest needs.</td>\n",
       "      <td id=\"T_89e4e_row6_col1\" class=\"data row6 col1\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row6_col2\" class=\"data row6 col2\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row6_col3\" class=\"data row6 col3\" >Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_89e4e_row7_col0\" class=\"data row7 col0\" >The expensive DSLR camera failed to meet my expectations. The autofocus is slow and inaccurate, resulting in many blurry shots. The battery life is disappointing, and the camera frequently overheats during extended use, making it unreliable for professional photography.</td>\n",
       "      <td id=\"T_89e4e_row7_col1\" class=\"data row7 col1\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row7_col2\" class=\"data row7 col2\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row7_col3\" class=\"data row7 col3\" >Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_89e4e_row8_col0\" class=\"data row8 col0\" >The online course on data science was a huge letdown. The lectures were poorly structured, with outdated content that didn't align with current industry standards. The assignments were vague, and the feedback from instructors was minimal and unhelpful.</td>\n",
       "      <td id=\"T_89e4e_row8_col1\" class=\"data row8 col1\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row8_col2\" class=\"data row8 col2\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row8_col3\" class=\"data row8 col3\" >Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89e4e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_89e4e_row9_col0\" class=\"data row9 col0\" >The high-end blender I purchased started emitting a burnt smell after a few uses. Despite its hefty price tag, it struggles to blend even soft fruits smoothly. The customer service was unresponsive to my complaints, leaving me with a defective product and no resolution.</td>\n",
       "      <td id=\"T_89e4e_row9_col1\" class=\"data row9 col1\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row9_col2\" class=\"data row9 col2\" >Negative</td>\n",
       "      <td id=\"T_89e4e_row9_col3\" class=\"data row9 col3\" >Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x149861890>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from safetensors.torch import load_file\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Loading the vocabulary\n",
    "def load_vocabulary(vocab_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab_path = '../../vocab/glove_model_vocab.txt'\n",
    "vocab = load_vocabulary(vocab_path)\n",
    "vocab['[CLS]'] = len(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Loading the embedding matrix\n",
    "def load_npy_embeddings(embedding_path, vocab):\n",
    "    embeddings = np.load(embedding_path)\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if idx < embeddings.shape[0]:\n",
    "            embedding_matrix[idx] = embeddings[idx]\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_path = '../../embeddings/glove_model_embeddings_300_d.npy'\n",
    "embedding_matrix = load_npy_embeddings(embedding_path, vocab)\n",
    "\n",
    "# Define the necessary model classes\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pretrained_embeddings=None):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.encoding = self.get_positional_encoding(max_len, embed_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_positional_encoding(max_len, embed_size):\n",
    "        encoding = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return encoding.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.encoding.size(0):\n",
    "            self.encoding = self.get_positional_encoding(seq_len, self.embed_size)\n",
    "        encoding = self.encoding[:seq_len, :].to(x.device)\n",
    "        return x + encoding.transpose(0, 1)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout_rate)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_size)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, num_heads, ff_dim, vocab_size, max_len, num_classes, dropout_rate=0.1):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(embed_size, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.dropout(x[:, 0, :])\n",
    "        return self.fc_out(x)\n",
    "\n",
    "pt_model_path = \"../../models/from_scratch_model/final_model_from_scratch.pth\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_pt = SentimentAnalysisModel(embed_size=300, num_layers=6, num_heads=12, ff_dim=1024, vocab_size=vocab_size, max_len=250, num_classes=2, dropout_rate=0.2)\n",
    "model_pt.load_state_dict(torch.load(pt_model_path, map_location=device))\n",
    "model_pt.eval()  \n",
    "model_pt.to(device)\n",
    "\n",
    "# Load the Hugging Face model\n",
    "hf_model_path = \"../../models/fine_tuned_model/model.safetensors\"\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "state_dict_hf = load_file(hf_model_path)\n",
    "model_hf = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", state_dict=state_dict_hf)\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and pad text\n",
    "def tokenize_and_pad_text(text, vocab, max_len=250):\n",
    "    tokens = preprocess_text(text)\n",
    "    tokenized = [vocab.get(word, vocab['[CLS]']) for word in tokens]\n",
    "    tokenized = tokenized[:max_len] + [0] * (max_len - len(tokenized)) if len(tokenized) < max_len else tokenized[:max_len]\n",
    "    return torch.tensor(tokenized, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Function to run inference\n",
    "def predict_sentiment(text, model, vocab, max_len=250):\n",
    "    tokenized_text = tokenize_and_pad_text(text, vocab, max_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        mask = None  # Assuming no mask for simplicity\n",
    "        output = model(tokenized_text, mask)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Loading the label encoder and fit it with the same labels used during training\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['negative', 'positive'])  # Ensure this matches your original training labels\n",
    "\n",
    "# Reviews for testing\n",
    "positive_reviews_1 = [\n",
    "    \"Great product!\",\n",
    "    \"Loved it!\",\n",
    "    \"Highly recommend.\",\n",
    "    \"Very satisfied.\",\n",
    "    \"Excellent quality.\",\n",
    "    \"Will buy again.\",\n",
    "    \"Perfect fit!\",\n",
    "    \"Amazing service.\",\n",
    "    \"Top-notch!\",\n",
    "    \"Exceeded expectations.\"\n",
    "]\n",
    "positive_reviews_2 = [\n",
    "    \"The delivery was prompt and the item was exactly as described.\",\n",
    "    \"Fantastic customer service, they resolved my issue quickly.\",\n",
    "    \"The sound quality of these headphones is phenomenal.\",\n",
    "    \"The fabric is soft and comfortable, great for everyday wear.\",\n",
    "    \"This book kept me engaged from start to finish, a must-read.\",\n",
    "    \"The hiking trail was well-marked and had breathtaking views.\",\n",
    "    \"The event was well-organized and very enjoyable.\",\n",
    "    \"The coffee maker brews perfect coffee every time.\",\n",
    "    \"The battery life on this laptop is impressive, lasts all day.\",\n",
    "    \"The tutorial videos were easy to follow and very informative.\"\n",
    "]\n",
    "positive_reviews_3 = [\n",
    "    \"The smartphone's camera capabilities are truly remarkable, capturing vivid colors and sharp details even in low-light conditions. The user interface is intuitive and the battery life exceeds my expectations, lasting well into the next day with heavy usage.\",\n",
    "    \"I attended a weekend workshop on digital marketing, and it was a game-changer for my business. The speakers were industry experts who shared invaluable insights and practical strategies that I could implement immediately.\",\n",
    "    \"The cruise vacation was an unforgettable experience. The ship was luxurious, with a variety of dining options, entertainment, and activities for all ages. The staff were incredibly attentive, and the shore excursions were well-planned and enriching.\",\n",
    "    \"The custom-built gaming PC I ordered exceeded my expectations in every way. The build quality is impeccable, with meticulously routed cables and high-end components that deliver top-tier performance in every game I've played so far.\",\n",
    "    \"The electric car not only provides a smooth and quiet ride but also offers impressive acceleration and a range that easily meets my daily commuting needs. The advanced driver-assistance features enhance safety, making every journey a pleasure.\"\n",
    "]\n",
    "\n",
    "negative_reviews_1 = [\n",
    "    \"Terrible experience.\",\n",
    "    \"Not as described.\",\n",
    "    \"Very disappointed.\",\n",
    "    \"Won't buy again.\",\n",
    "    \"Poor quality.\",\n",
    "    \"Too expensive.\",\n",
    "    \"Didn't like it.\",\n",
    "    \"Waste of money.\",\n",
    "    \"Not worth it.\",\n",
    "    \"Bad service.\"\n",
    "]\n",
    "negative_reviews_2 = [\n",
    "    \"The software is slow and crashes frequently, making it very frustrating to use.\",\n",
    "    \"The hotel's location is inconvenient, far from major attractions and public transportation.\",\n",
    "    \"The jacket started falling apart after just a few wears, very disappointed with the quality.\",\n",
    "    \"The concert venue was overcrowded and the acoustics were terrible.\",\n",
    "    \"The restaurant service was extremely slow, and the food was bland and overpriced.\",\n",
    "    \"The vacuum cleaner is bulky and difficult to maneuver, not practical for daily use.\",\n",
    "    \"The movie was poorly written and the acting was subpar, not worth the ticket price.\",\n",
    "    \"The customer service hotline is always busy, and it takes forever to get a response.\",\n",
    "    \"The app is filled with bugs and the user interface is not intuitive at all.\",\n",
    "    \"The subscription box had repetitive items and didn't match my preferences.\"\n",
    "]\n",
    "\n",
    "negative_reviews_3 = [\n",
    "    \"The new version of the software is riddled with bugs that significantly hamper productivity. It crashes frequently, and the user interface changes have made navigation cumbersome. The promised new features are either missing or don't work as advertised.\",\n",
    "    \"I recently stayed at a highly-rated resort, but my experience was far from pleasant. The room was not cleaned properly, with visible dust and stains on the sheets. The food at the on-site restaurant was mediocre, and the staff seemed indifferent to guest needs.\",\n",
    "    \"The expensive DSLR camera failed to meet my expectations. The autofocus is slow and inaccurate, resulting in many blurry shots. The battery life is disappointing, and the camera frequently overheats during extended use, making it unreliable for professional photography.\",\n",
    "    \"The online course on data science was a huge letdown. The lectures were poorly structured, with outdated content that didn't align with current industry standards. The assignments were vague, and the feedback from instructors was minimal and unhelpful.\",\n",
    "    \"The high-end blender I purchased started emitting a burnt smell after a few uses. Despite its hefty price tag, it struggles to blend even soft fruits smoothly. The customer service was unresponsive to my complaints, leaving me with a defective product and no resolution.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_reviews = positive_reviews_3 + negative_reviews_3\n",
    "# Tokenize and prepare inputs for Hugging Face model\n",
    "inputs_hf = tokenizer_hf(all_reviews, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# Getting predictions from Hugging Face model\n",
    "with torch.no_grad():\n",
    "    outputs_hf = model_hf(**inputs_hf)\n",
    "    predictions_hf = torch.argmax(outputs_hf.logits, dim=-1)\n",
    "# Getting predictions from the from-scratch model\n",
    "predictions_pt = [predict_sentiment(review, model_pt, vocab) for review in all_reviews]\n",
    "labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "results = {\n",
    "    \"Review\": all_reviews,\n",
    "    \"Expected Sentiment\": [\"Positive\"] * len(positive_reviews_3) + [\"Negative\"] * len(negative_reviews_3),\n",
    "    \"Fine-tuned BERT\": [labels[pred.item()] for pred in predictions_hf],\n",
    "    \"From-scratch Model\": [labels[pred] for pred in predictions_pt]\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "styled_results = df_results.style.set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n",
    "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    ").set_properties(**{'text-align': 'left'})\n",
    "styled_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e9a1b-48a6-4dc0-80d0-07b8ca1e7c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
