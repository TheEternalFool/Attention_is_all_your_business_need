{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ac59b-df2f-42d1-aefe-51c99e9cb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Loading the vocabulary\n",
    "def load_vocabulary(vocab_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab_path = 'glove_model_vocab.txt'\n",
    "vocab = load_vocabulary(vocab_path)\n",
    "# Adding the [CLS] token to the vocabulary\n",
    "vocab['[CLS]'] = len(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Loading the embedding matrix\n",
    "def load_npy_embeddings(embedding_path, vocab):\n",
    "    embeddings = np.load(embedding_path)\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if idx < embeddings.shape[0]:\n",
    "            embedding_matrix[idx] = embeddings[idx]\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_path = 'glove_model_embeddings_300_d.npy'\n",
    "embedding_matrix = load_npy_embeddings(embedding_path, vocab)\n",
    "\n",
    "# Defining the embedding layer class\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pretrained_embeddings=None):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# Defining a class for adding positional encoding to the embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.encoding = self.get_positional_encoding(max_len, embed_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_positional_encoding(max_len, embed_size):\n",
    "        encoding = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return encoding.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.encoding.size(0):\n",
    "            self.encoding = self.get_positional_encoding(seq_len, self.embed_size)\n",
    "        encoding = self.encoding[:seq_len, :].to(x.device)  # Ensure encoding is on the same device as x\n",
    "        return x + encoding.transpose(0, 1)\n",
    "\n",
    "# Defining the Encoder block class\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout_rate)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_size)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "\n",
    "# Defining the Sentiment Analysis model class\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, num_heads, ff_dim, vocab_size, max_len, num_classes, dropout_rate):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderBlock(embed_size, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1)  # Convert to (seq_len, batch_size, embed_size) for attention layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        x = x.transpose(0, 1)  # Convert back to (batch_size, seq_len, embed_size)\n",
    "        x = self.dropout(x[:, 0, :])  # Use the encoding of the first token ([CLS] token) for classification\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# Data preprocessing\n",
    "class SentimentPreprocessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Lowercase the text\n",
    "        self.data['review'] = self.data['review'].str.lower()\n",
    "\n",
    "        # Removing punctuation\n",
    "        self.data['review'] = self.data['review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "        # Removing stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        self.data['review'] = self.data['review'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "        # Tokenizing the text\n",
    "        self.data['review'] = self.data['review'].apply(word_tokenize)\n",
    "\n",
    "        return self.data\n",
    "\n",
    "def tokenize_and_pad(df, vocab, max_len):\n",
    "    def tokenize_review(review, vocab):\n",
    "        return [vocab.get(word, vocab['[CLS]']) for word in review]\n",
    "\n",
    "    df['tokenized'] = df['review'].apply(lambda x: tokenize_review(x, vocab))\n",
    "    df['tokenized'] = df['tokenized'].apply(lambda x: x[:max_len] + [0] * (max_len - len(x)) if len(x) < max_len else x[:max_len])\n",
    "    return df\n",
    "\n",
    "preprocessor = SentimentPreprocessor('IMDB Dataset.csv')\n",
    "df_preprocessed = preprocessor.preprocess()\n",
    "label_encoder = LabelEncoder()\n",
    "df_preprocessed['label'] = label_encoder.fit_transform(df_preprocessed['sentiment'])\n",
    "max_len = 250  # Ensure max_len is consistent across preprocessing and model\n",
    "df_tokenized = tokenize_and_pad(df_preprocessed, vocab, max_len)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = torch.tensor(self.data.iloc[idx]['tokenized'], dtype=torch.long)\n",
    "        label = torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long)\n",
    "        return review, label\n",
    "\n",
    "dataset = IMDBDataset(df_tokenized)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Model configuration\n",
    "embed_size = 300\n",
    "num_layers = 6\n",
    "num_heads = 12\n",
    "ff_dim = 1024\n",
    "vocab_size = len(vocab)\n",
    "num_classes = 2\n",
    "dropout_rate = 0.2\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# Initialize lists to store results\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "fold_val_accuracies = []\n",
    "\n",
    "all_fold_labels = []\n",
    "all_fold_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    train_subsampler = Subset(dataset, train_idx)\n",
    "    val_subsampler = Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model, criterion, optimizer, and scheduler\n",
    "    model = SentimentAnalysisModel(embed_size, num_layers, num_heads, ff_dim, vocab_size, max_len, num_classes, dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    # Training and Validation\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    fold_labels = []\n",
    "    fold_preds = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for reviews, labels in train_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "            mask = None  # Assuming no mask for simplicity\n",
    "            outputs = model(reviews, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for reviews, labels in val_loader:\n",
    "                reviews, labels = reviews.to(device), labels.to(device)\n",
    "                mask = None  # Assuming no mask for simplicity\n",
    "                outputs = model(reviews, mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        fold_labels.extend(all_labels)\n",
    "        fold_preds.extend(all_preds)\n",
    "\n",
    "        scheduler.step(val_loss / len(val_loader))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'best_model_fold_{fold + 1}.pth')\n",
    "    \n",
    "    fold_train_losses.append(train_losses)\n",
    "    fold_val_losses.append(val_losses)\n",
    "    fold_val_accuracies.append(val_accuracies)\n",
    "\n",
    "    all_fold_labels.extend(fold_labels)\n",
    "    all_fold_preds.extend(fold_preds)\n",
    "\n",
    "average_train_loss = np.mean([np.mean(losses) for losses in fold_train_losses])\n",
    "average_val_loss = np.mean([np.mean(losses) for losses in fold_val_losses])\n",
    "average_val_accuracy = np.mean([np.mean(accuracies) for accuracies in fold_val_accuracies])\n",
    "\n",
    "print(f'Average Train Loss: {average_train_loss:.4f}')\n",
    "print(f'Average Val Loss: {average_val_loss:.4f}')\n",
    "print(f'Average Val Accuracy: {average_val_accuracy:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(fold_train_losses, axis=0), label='Average Training Loss')\n",
    "plt.plot(np.mean(fold_val_losses, axis=0), label='Average Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(fold_val_accuracies, axis=0), label='Average Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_fold_labels, all_fold_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(all_fold_labels, all_fold_preds, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
